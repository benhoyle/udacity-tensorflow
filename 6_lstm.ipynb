{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // batch_size\n",
    "        self._cursor = [ offset * segment for offset in range(batch_size)] # this sets 64 cursor starting indexes \n",
    "        # for the text\n",
    "        self._last_batch = self._next_batch()\n",
    "  \n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "        batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float) # initialise a matrix of 64 * 27\n",
    "        # I.e. each batch in this case involves 64 different sets of character sequences.\n",
    "        for b in range(self._batch_size): # repeat for each of these 64 character sequences\n",
    "            batch[b, char2id(self._text[self._cursor[b]])] = 1.0 # for each row representing a sequence in the batch \n",
    "            # set the index in the character array to one based on the character in the text at the cursor for\n",
    "            # the particular batch\n",
    "            # Then increment the cursor for the particular batch\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size # % just enables cycling of data\n",
    "        return batch\n",
    "  \n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "            self._last_batch = batches[-1]\n",
    "        return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "    return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "    \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "    representation.\"\"\"\n",
    "    s = [''] * batches[0].shape[0] # s is list with 64 blank entries \n",
    "    for b in batches: # for each of 11 batches\n",
    "        # characters(b) is a list of 64 characters - i.e. batch > characters\n",
    "        s = [''.join(x) for x in zip(s, characters(b))] # joins batch characters vertically\n",
    "    return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "So a valid batch contains two characters - one for the input, one for the output which is the next character in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batches = train_batches.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s = [''] * batches[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = np.concatenate(list(batches)[1:])\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "c = characters(batches[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "s = [''.join(x) for x in zip(s, c)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'm',\n",
       " 'a',\n",
       " 'i',\n",
       " 'e',\n",
       " ' ',\n",
       " 'g',\n",
       " 'g',\n",
       " 'a',\n",
       " ' ',\n",
       " 'k',\n",
       " 's',\n",
       " 's',\n",
       " 'e',\n",
       " 'z',\n",
       " ' ',\n",
       " ' ',\n",
       " 'h',\n",
       " 'g',\n",
       " ' ',\n",
       " 'n',\n",
       " 'c',\n",
       " 't',\n",
       " 'c',\n",
       " 'u',\n",
       " 'o',\n",
       " 't',\n",
       " ' ',\n",
       " 't',\n",
       " 'o',\n",
       " 's',\n",
       " 'g',\n",
       " 'r',\n",
       " 'e',\n",
       " 'r',\n",
       " 'h',\n",
       " 'a',\n",
       " ' ',\n",
       " 's',\n",
       " 'l',\n",
       " 'a',\n",
       " ' ',\n",
       " ' ',\n",
       " 'e',\n",
       " 'h',\n",
       " 'o',\n",
       " 'n',\n",
       " 'i',\n",
       " ' ',\n",
       " 'r',\n",
       " 'e',\n",
       " 's',\n",
       " 'a',\n",
       " 'o',\n",
       " 'i',\n",
       " 't',\n",
       " 'i',\n",
       " 'w',\n",
       " 'e',\n",
       " 'c',\n",
       " 'e',\n",
       " ' ',\n",
       " ' ',\n",
       " 'v']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Each call to next returns a list of 11 batches (the last batch plus 10 unrollings).  \n",
    "\n",
    "Each batch has 64 characters? Next batch contains the next character for each of those 64 characters. This is repeated 10 times to yield 11 batchs: a last set of characters and 10 iterations forward through the sequence of text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "How next batch works - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "# Logprob is used to compute the perplexity\n",
    "def logprob(predictions, labels):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "    \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "    probabilities.\n",
    "    \"\"\"\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model.  \n",
    "To help understand what this is doing see:  \n",
    "* http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "* https://www.tensorflow.org/tutorials/recurrent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This is the input size, which equals 26 lower case characters plus space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "What is num_nodes?  - Size of the hidden dimension of the cell, or the number of units in the LSTM cell as per here - http://monik.in/a-noobs-guide-to-implementing-rnn-lstm-using-tensorflow/ .\n",
    "  \n",
    "Where does the classifier come in with w and b?  \n",
    "\n",
    "The cell has 'state' (C) and 'output' (h or y).  \n",
    "\n",
    "Input gate is a set of weights applied to the current input and the previous output + bias. ix below is the part of the weights applied to the input and im is the part of the weights applied to the previous output.  \n",
    "\n",
    "Classifier is applied to the output of the trained cell? Yes - even more so it is applied to the outputs of a bunch of cells equal to the number of unrollings? \n",
    "\n",
    "The LSTM cell takes a character as an input and outputs a high dimensional vector of length num_nodes. Each cell produces an output of dimension num_nodes = 64. The classifier takes this hidden output and generates a character prediction based on its own weights and bias (w and b).\n",
    "\n",
    "The classifier tries to predict an output based on the last num_unrolling outputs? No. Just on a LSTM cell output.\n",
    "\n",
    "We have train_data and train_inputs - how do these differ? Train data is used to generate inputs and labels (labels are just the inputs shifted by one along the character axis).\n",
    "\n",
    "Number of unrollings = 10. Batch size = 64. \n",
    "\n",
    "Unrollings = number of characters in the history to look at?\n",
    "\n",
    "Train_data is a list of 11 entries each of shape 64 x 27. It is 11 as the first 10 are the inputs and the labels are the following 10 inputs. Each input is a matrix of shape 64 x 27 (batch size x vocab size) and each label is a matrix of the same size that represents the next set of inputs in the sequence. I.e. the input comprises 64 rows of vectors indicating a character.\n",
    "\n",
    "For each input, the output and the state of the LSTM is computed. The output is added to a list of outputs. Outputs are then compared with the labels? No, outputs are fed into a classifier - output of the classifier indicates predicted character.\n",
    "\n",
    "Each output is a matrix of size batch size x hidden layers?\n",
    "\n",
    "In particular, outputs are concatenated along rows (axis 0). Outputs (the list) has num_unrollings, or 10 entries. Outputs are of length = number of hidden layers/units/nodes.\n",
    "\n",
    "When we train the classifier we supply in parallel all the batches for the 10 unrollings in one matrix of training data. So the training data can be thought of as 64 batches of 10 unrollings in sequence. Trying to match sequences of 10 movements through the text, with input and output characters at each step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "How do saved_output and saved_state work? Used in the control_dependencies portion.\n",
    "\n",
    "Bit below control_dependencies will only run after saved_output.assign(output) and                   saved_state.assign(state) have been evaluated - see https://www.tensorflow.org/api_docs/python/tf/Graph#control_dependencies. This just saves the last output and state, which is used the next time around."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Concat along axis 0 provides one dimension of output samples (e.g. rows of samples wherein the columns are the output dimensions - length batch size - i.e. 64), which is compared with one dimension of label samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    }
   ],
   "source": [
    "print(vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n"
     ]
    }
   ],
   "source": [
    "train_data = list()\n",
    "for _ in range(num_unrollings + 1):\n",
    "    train_data.append(_)\n",
    "train_inputs = train_data[:num_unrollings]\n",
    "train_labels = train_data[1:]\n",
    "print(train_inputs, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This is a good walk-through: https://iamtrask.github.io/2015/11/15/anyone-can-code-lstm/ .\n",
    "\n",
    "Human brain has 3-7 'slots' for recent information but can chunk hierarchically to remember further back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "    # Parameters:\n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Memory cell: input, state and bias.                             \n",
    "    cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "        forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "        update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "            tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "          tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    # Apply max clipping of gradients\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "        sample_input, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "num_steps is also known as epoch number.  \n",
    "\n",
    "In each epoch, we train over a set of batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.297829 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.05\n",
      "================================================================================\n",
      "bnvhooauoampigtkjwppik p ui ypikqirrxjmmilsndowlvlsi h omceyst astbkxadclarletyp\n",
      "pnethc sjrvhlsi xri mlsdmsmhogzyddhcdoubaime lhooyen  agfmt stdjyxuzmoynzta zicu\n",
      "strx vcklcz bbcdhzoqecno lavrgdone n ed a atwbfnonose oee bxtr hzpnlwxsecaokffu \n",
      "t aamttsz osvqriiatetpotreteeqckr mohijeha nvaah dusaeqtqnhfwpd iwihuextnjwkome \n",
      "mpfeeteu  h eeprtshjnzlo awu e skkts dq a floamfx prkehbyncuttrrnlbtnjdcae tpuyk\n",
      "================================================================================\n",
      "Validation set perplexity: 19.97\n",
      "Average loss at step 100: 2.624575 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.07\n",
      "Validation set perplexity: 10.60\n",
      "Average loss at step 200: 2.259818 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.67\n",
      "Validation set perplexity: 9.32\n",
      "Average loss at step 300: 2.103225 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.59\n",
      "Validation set perplexity: 8.03\n",
      "Average loss at step 400: 2.002821 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.53\n",
      "Validation set perplexity: 7.57\n",
      "Average loss at step 500: 1.936773 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.30\n",
      "Validation set perplexity: 7.07\n",
      "Average loss at step 600: 1.912801 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.21\n",
      "Validation set perplexity: 6.80\n",
      "Average loss at step 700: 1.857657 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 6.72\n",
      "Average loss at step 800: 1.821570 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.27\n",
      "Validation set perplexity: 6.56\n",
      "Average loss at step 900: 1.829934 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.20\n",
      "Validation set perplexity: 6.19\n",
      "Average loss at step 1000: 1.822710 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.09\n",
      "================================================================================\n",
      "y in this turning his ears offerty ovin ivind suched to the distatury n sing in \n",
      "or procemer ffomoriancomen in more and w an one nine five a the cervibo it mikil\n",
      " the kintine as witina preece and from for was linaraica to the forbed mideratio\n",
      " and five sictarus by the mard of the girferic descuiled simutions resoukd in di\n",
      "pberan s d pares was laymempopfor resal sopidios linfitism bystientatist fricerd\n",
      "================================================================================\n",
      "Validation set perplexity: 6.04\n",
      "Average loss at step 1100: 1.777886 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 5.94\n",
      "Average loss at step 1200: 1.750547 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.15\n",
      "Validation set perplexity: 5.62\n",
      "Average loss at step 1300: 1.730314 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.83\n",
      "Validation set perplexity: 5.66\n",
      "Average loss at step 1400: 1.741790 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 5.58\n",
      "Average loss at step 1500: 1.738325 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.31\n",
      "Validation set perplexity: 5.47\n",
      "Average loss at step 1600: 1.746314 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 5.32\n",
      "Average loss at step 1700: 1.711228 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 5.42\n",
      "Average loss at step 1800: 1.673303 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 1900: 1.648366 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.11\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 2000: 1.693805 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.88\n",
      "================================================================================\n",
      "t with nich as bok a time bewall wren ineline that will futronimally sidion cinc\n",
      "ar s is sibled eight three and colled home lenzagas his the sudded gincped is al\n",
      "bamgy tajes respace it the and of zlasing roc zero zero s su oper a thoup he has\n",
      "y the unitann struges at is insuected in gording and work c at the mad tneava us\n",
      "il shikn ka cial a sithin the woush he informignality indeared s birel any to su\n",
      "================================================================================\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 2100: 1.686867 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 2200: 1.678413 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 2300: 1.637739 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 2400: 1.656644 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 2500: 1.678674 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.87\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 2600: 1.655203 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 2700: 1.655843 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 2800: 1.648115 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 2900: 1.655239 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 3000: 1.653392 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.76\n",
      "================================================================================\n",
      "kation with prothand which bevisy as wibntory av and strobarly lond hotthone liz\n",
      "ing he leter ruler euger operation in the leady wayty iffect surration casic wel\n",
      "greato protonational opricial most stron are readicall pect one nine six sucters\n",
      "cound for early the ballagy and with terman ay statine purcle into in new hiskab\n",
      "werre a klading havey perpered and long in at tspetink have in arruc to coulbret\n",
      "================================================================================\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 3100: 1.628502 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 3200: 1.644509 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 3300: 1.639716 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.75\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 3400: 1.669048 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.15\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 3500: 1.653934 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 3600: 1.665067 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 3700: 1.647500 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 3800: 1.647260 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 3900: 1.640207 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 4000: 1.651031 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.51\n",
      "================================================================================\n",
      "phiding his games at the swain bythams however rpll availed halvaed peroor tfun \n",
      "x of bardination often socether allancollosispyant contree complosolus and iu sc\n",
      "ur reperfedisted the lass roundy propas of the early yom when imperiant canase j\n",
      "co tlintions drowing of the few vive ameraple like government reclencibutions of\n",
      "co president in need retrictly but jois rennith a success dessevent of sanizinca\n",
      "================================================================================\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 4100: 1.634018 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 4200: 1.633935 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 4300: 1.616769 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 4400: 1.606337 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 4500: 1.613365 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 4600: 1.615680 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 4700: 1.625145 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 4800: 1.629646 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 4900: 1.631718 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 5000: 1.607001 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.25\n",
      "================================================================================\n",
      "gress of the possicicary tuss that opering of posates and in the partial s mains\n",
      "de seeb blloganal d one impoph it to and cleago on in of gragians and bounds in \n",
      "child on ethat of moneentian seven chetubble and place moge the trad one of the \n",
      "y jinigo convent varial gred tumral both granited and he sukal was dipthroot ack\n",
      "le and imot ruling additical pricats accords park waves are matique entesps nept\n",
      "================================================================================\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 5100: 1.605741 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 5200: 1.589437 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 5300: 1.578402 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 5400: 1.578576 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 5500: 1.569640 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 5600: 1.579791 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 5700: 1.569575 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 5800: 1.580980 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 5900: 1.574349 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 6000: 1.546468 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "================================================================================\n",
      " has at that now the world imally reparts and comed in story of john make some j\n",
      "d substance on larged as tran fanote is a scandals of reservially s suppricate q\n",
      "rewing bloom bc damgalighived cophimate not chanimenting moclariquent performami\n",
      "rensed however reportainpes charidy exait ie the guisfergomem scheigality earth \n",
      "quine prime are but often as benguatter that up a tiopated in three two jameal f\n",
      "================================================================================\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 6100: 1.563521 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.18\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 6200: 1.541267 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 6300: 1.540197 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 6400: 1.539804 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 6500: 1.556502 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 6600: 1.594992 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 6700: 1.576442 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.42\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 6800: 1.602367 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 6900: 1.579945 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 7000: 1.578391 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "================================================================================\n",
      "dectife serves datas and depicial poolded var compute with the misiturgal has en\n",
      "ffling rudertualled of when highop and charic brounds ricelva in organa sicking \n",
      "revinety of the chn its hregsuda undor affario ef movicaty and in algoin is the \n",
      "dayion long yave ge mahaniev southind a one nine nine six demeet senced thele in\n",
      "y parce or fexemb depatuasfarimatiesta k lembilloid off  arsurachi one seven nin\n",
      "================================================================================\n",
      "Validation set perplexity: 4.34\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        # Each step has a different batch\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        # This loads the train_data with the last batch + 10 unrollings\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        # This runs the training\n",
    "        _, l, predictions, lr = session.run(\n",
    "                  [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        # Runs every 100 iterations\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "                  # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print(\n",
    "                'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            # Below creates one row of all the labels\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(\n",
    "                np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "                valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Input matrix multiplications are:\n",
    "* tf.matmul(i, ix)\n",
    "* tf.matmul(i, fx)\n",
    "* tf.matmul(i, cx)\n",
    "* tf.matmul(i, ox)  \n",
    "\n",
    "Output matrix multiplications are:\n",
    "* tf.matmul(o, im)\n",
    "* tf.matmul(o, fm)\n",
    "* tf.matmul(o, cm)\n",
    "* tf.matmul(o, om)\n",
    "\n",
    "i has dimensions :, vocab_size  \n",
    "o has dimensions :, num_nodes  \n",
    "\n",
    "ix, fx, cx, and ox are all of size: vocab_size, num_nodes  \n",
    "im, fm, cm, and om are all of size: num_nodes, num_nodes  \n",
    "\n",
    "So we make two matrices: x and m?  \n",
    "x size: vocab_size, num_nodes\\*4  \n",
    "m size: num_nodes, num_nodes\\*4  \n",
    "\n",
    "Then we just slice result into num_nodes segments?  \n",
    "\n",
    "input_matmul = tf.matmul(i, x)\n",
    "output_matmul = tf.matmul(o, m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "    # Parameters:\n",
    "    x = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes*4], -0.1, 0.1))\n",
    "    m = tf.Variable(tf.truncated_normal([num_nodes, num_nodes*4], -0.1, 0.1))\n",
    "    # Input gate: bias.\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Forget gate: bias.   \n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Memory cell: bias.                               \n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Output gate: bias.\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        input_matmul = tf.matmul(i, x)\n",
    "        output_matmul = tf.matmul(o, m)\n",
    "        input_gate = tf.sigmoid(input_matmul[:, :num_nodes] \n",
    "                                + output_matmul[:, :num_nodes] \n",
    "                                + ib)\n",
    "        forget_gate = tf.sigmoid(input_matmul[:, num_nodes:num_nodes*2] \n",
    "                                 + output_matmul[:, num_nodes:num_nodes*2] \n",
    "                                 + fb)\n",
    "        update = (input_matmul[:, num_nodes*2:num_nodes*3] \n",
    "                  + output_matmul[:, num_nodes*2:num_nodes*3]  \n",
    "                  + cb)\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(input_matmul[:, num_nodes*3:]\n",
    "                                 + output_matmul[:, num_nodes*3:] \n",
    "                                 + ob)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "            tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "          tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    # Apply max clipping of gradients\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "        sample_input, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.297819 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.05\n",
      "================================================================================\n",
      "se aatruejmqu twzpz qtw trzel tk dh y ltfsyqiz dfjt giegzveg e qdrrupei olq e mx\n",
      "pj ejjohp k eeoi ri  pvkvwlohstpg eerfe odqddoi llyhyrileof  oswogtneey  hoaaeqw\n",
      "ynuppxknoghwmtjjbndmrswn  fsctoplhf psfedujt  lwh qheotdwas w meeahexej y rcbiri\n",
      "bnkxs ttrqv  arjjahszqwayavztx u szdgjirjstyyofneeecmhy zzuwz  klfdgwlgdexogdevs\n",
      "qrsjeqtf tfskg  nxmveiasyanjxxlzarrs neiigeafxxhtott  elrgbmublwcwjnuqmhg ji nfv\n",
      "================================================================================\n",
      "Validation set perplexity: 20.17\n",
      "Average loss at step 100: 2.588457 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.50\n",
      "Validation set perplexity: 10.59\n",
      "Average loss at step 200: 2.242242 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.06\n",
      "Validation set perplexity: 9.42\n",
      "Average loss at step 300: 2.083490 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.19\n",
      "Validation set perplexity: 8.22\n",
      "Average loss at step 400: 1.993982 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.26\n",
      "Validation set perplexity: 8.15\n",
      "Average loss at step 500: 1.995258 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.58\n",
      "Validation set perplexity: 7.35\n",
      "Average loss at step 600: 1.924773 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.99\n",
      "Validation set perplexity: 6.79\n",
      "Average loss at step 700: 1.892307 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.10\n",
      "Validation set perplexity: 6.88\n",
      "Average loss at step 800: 1.870772 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.04\n",
      "Validation set perplexity: 6.51\n",
      "Average loss at step 900: 1.856003 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 6.36\n",
      "Average loss at step 1000: 1.790874 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "================================================================================\n",
      "ectlot comport mepuled in a stack ox kimpling raigen of laigian one nine nine th\n",
      "y dained kisturalpstave in the ninal and dicce sinctual nonvermituels is pretinu\n",
      "f but new myations with fain lo and secrinctineal inde baymbatund and modulad ju\n",
      "am we wickinisymitigns higher candophater pose ficts this munes hary for laged w\n",
      "or were binath in the eart chiritwas ach a continient for ungo ored two the uple\n",
      "================================================================================\n",
      "Validation set perplexity: 6.20\n",
      "Average loss at step 1100: 1.766443 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.65\n",
      "Validation set perplexity: 6.33\n",
      "Average loss at step 1200: 1.789321 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.66\n",
      "Validation set perplexity: 6.30\n",
      "Average loss at step 1300: 1.768418 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.79\n",
      "Validation set perplexity: 5.87\n",
      "Average loss at step 1400: 1.738723 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 5.84\n",
      "Average loss at step 1500: 1.728200 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.80\n",
      "Validation set perplexity: 5.70\n",
      "Average loss at step 1600: 1.715738 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 5.73\n",
      "Average loss at step 1700: 1.740449 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.75\n",
      "Validation set perplexity: 5.66\n",
      "Average loss at step 1800: 1.706922 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 5.61\n",
      "Average loss at step 1900: 1.706111 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.10\n",
      "Validation set perplexity: 5.38\n",
      "Average loss at step 2000: 1.715122 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "================================================================================\n",
      "fre no wishwervially one nine nine four awarded koral kicle defer strkard bare h\n",
      "toral at michiluted both corrount rato units was collide eves strals tratish his\n",
      "k drad one sixed dighiutical product the trove the others proworn agal accortarc\n",
      "x the our shord convertose scheade sich of bren bs p it awir zad more not win do\n",
      " jef the plater the that the dicm in that wave the brown ghe the um a carlate it\n",
      "================================================================================\n",
      "Validation set perplexity: 5.48\n",
      "Average loss at step 2100: 1.700607 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.75\n",
      "Validation set perplexity: 5.40\n",
      "Average loss at step 2200: 1.674620 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 5.23\n",
      "Average loss at step 2300: 1.682385 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 2400: 1.683810 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 5.15\n",
      "Average loss at step 2500: 1.704856 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 2600: 1.675744 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 5.28\n",
      "Average loss at step 2700: 1.691128 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 2800: 1.652676 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 5.09\n",
      "Average loss at step 2900: 1.659548 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 3000: 1.662492 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.97\n",
      "================================================================================\n",
      "xmins windna it a solan many that the tisl to will continue that the loth and we\n",
      "ourbed not the searngysil menstics can liminani arvo ire is and man beep bic is \n",
      "crapted became in the gun livib somesas an u esmand usets his forming researing \n",
      "zes tim pany to by umposese yot in can orden esternafly snex that more and one b\n",
      "les creek balruloishant delovolph universofm h loanshiny seven of last the inccu\n",
      "================================================================================\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 3100: 1.651020 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 3200: 1.652869 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 3300: 1.636350 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 3400: 1.632570 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 3500: 1.626733 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 3600: 1.632547 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 3700: 1.632405 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.88\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 3800: 1.622024 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 3900: 1.616033 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 4000: 1.616658 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.70\n",
      "================================================================================\n",
      "wisst ordered foundif and outa in the batthive general crisofthys within is the \n",
      " s they het jens as instern ulfic ols he a prold snovemberhing weet etc been one\n",
      "k becounded informstic seterater with musiction yearster on upsire ships oonires\n",
      "on and wh the enmit shits new mateonakhhan one in wwances prescessan entradia mi\n",
      "was clactions and pedinisted the obidests and weur in the name only te perenti s\n",
      "================================================================================\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 4100: 1.619438 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 4200: 1.602477 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.07\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 4300: 1.590585 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 4400: 1.617445 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 4500: 1.625206 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 4600: 1.625604 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.75\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 4700: 1.594401 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 4800: 1.576592 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 4900: 1.593164 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 5000: 1.616511 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.70\n",
      "================================================================================\n",
      "was or are desagout is one sudcigctives green pare to nucinning was as the whate\n",
      "ly to fire ratite if ayol projects which grans rodieveakent in the  from atcomat\n",
      "x fasque is fucl when whate which a idete tiles as aisends sech after yerved deb\n",
      "zon eight seven deaving late catain have footbortes and with normay as viadbe tw\n",
      "m hadizant spised cellon identical it during that jurises dispinented bockspice \n",
      "================================================================================\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 5100: 1.628107 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 5200: 1.627188 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 5300: 1.591041 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 5400: 1.587543 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 5500: 1.574547 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 5600: 1.603835 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 5700: 1.564960 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 5800: 1.568982 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 5900: 1.590076 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 6000: 1.555501 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "================================================================================\n",
      "bominations ollenmed inem whthe develoge of the uren bus known as a forsed a pet\n",
      "ker tenging only a arean reducks before and than tubble new of absuffence of ins\n",
      "nify by fime is roux by nide afrgok indumstamas arcondisys one the problem bim t\n",
      "uger tegerosism are neve three conferged withoutmers of ottwo and such over of t\n",
      "gentatiana honory s adorbi vairs of malgatic changing repraise of substance clas\n",
      "================================================================================\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 6100: 1.574433 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 6200: 1.590979 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 6300: 1.605356 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 6400: 1.630027 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 6500: 1.632695 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 6600: 1.599276 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 6700: 1.585826 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 6800: 1.571934 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 6900: 1.568085 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 7000: 1.580405 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "================================================================================\n",
      "ary thore house ybann belineenth tomell uma marrands ucing distankbal chole a ba\n",
      "anced but innel not kangrevate marious for known barabon ru belewival in first t\n",
      "ated experts playspafter of a nuves of his mologo which in there cra chemige der\n",
      "festru ghosuling and franco my shisongs genesty this psena and nbylal gaster u b\n",
      "wer ewevel fooring archiles fe widled one zero zero in signinuals acrainted the \n",
      "================================================================================\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 7100: 1.548106 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 7200: 1.591610 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 7300: 1.593076 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 7400: 1.578259 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 7500: 1.555415 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 7600: 1.577091 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 7700: 1.571018 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 7800: 1.570583 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 7900: 1.581340 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 8000: 1.552160 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.22\n",
      "================================================================================\n",
      "zan quir leching lable can wable terms off trate legate have for soquects enovin\n",
      "or maris out also its econce but s stations concept essees explosion of his tron\n",
      "os drive of this and precience both the invention bilters american the bekine de\n",
      "tor product ree trecroub fl morg off in two zero in the adain and war most song \n",
      "tors historolies gold of short england claim was quinable sour of ishe graely po\n",
      "================================================================================\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 8100: 1.570178 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 8200: 1.577090 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 8300: 1.584978 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 8400: 1.566732 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 8500: 1.562555 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.20\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 8600: 1.545824 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.18\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 8700: 1.558247 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 8800: 1.581976 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 8900: 1.572285 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 9000: 1.582330 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.53\n",
      "================================================================================\n",
      "netachy sroge life had pore chrocles australiest greech of g as the ministory at\n",
      "s seven sater the species same s or in much low linasosts kinoking i elemino pas\n",
      "ge the creating naturallism for the considered of may of cluber can bryndmemian \n",
      " and most of nocule signal and with rond lange sugen three nine the commits butt\n",
      "bed the particely been four one also but becarreds on example of which awean he \n",
      "================================================================================\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 9100: 1.578970 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 9200: 1.591945 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.83\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 9300: 1.578693 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.28\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 9400: 1.573647 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 9500: 1.577804 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 9600: 1.579285 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 9700: 1.575994 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 9800: 1.571298 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 9900: 1.577284 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 10000: 1.596349 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.56\n",
      "================================================================================\n",
      "jynic permaniding packfachil andions spetic and nation usero dina of memists don\n",
      "que facrent are only one et three one five and seven a turk the centrally isan o\n",
      "que they eart the cholanding being lowbles the chystingion polagonies or to sbyd\n",
      "aghm concessiut the one one two introduce a while only by two zin chinuar a that\n",
      "us socuellar chapand the leeple at q murical profess sune at ato life three man \n",
      "================================================================================\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 10100: 1.563106 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 10200: 1.577218 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 10300: 1.575056 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 10400: 1.575028 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 10500: 1.565477 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 10600: 1.585568 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 10700: 1.560508 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 10800: 1.556060 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 10900: 1.551886 learning rate: 0.100000\n",
      "Minibatch perplexity: 6.02\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 11000: 1.579903 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.44\n",
      "================================================================================\n",
      "che buvbloi cander isolisian occus type s addectining mirica ang called ty in on\n",
      "ing the offchinger but triatek in the six and complelties and of nams had his su\n",
      "x the latbers annoum the stara liter with g arriculam his recause theory impolat\n",
      "org blomests of the for and wristing starplands having rallas is robert to trigh\n",
      "wassts mynization hole mad ulovally in evoldow and memp discodition near dock bo\n",
      "================================================================================\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 11100: 1.585707 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 11200: 1.575018 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 11300: 1.537002 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 11400: 1.563230 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 11500: 1.558822 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 11600: 1.551183 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 11700: 1.567698 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 11800: 1.572993 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.44\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 11900: 1.600674 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 12000: 1.592136 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.67\n",
      "================================================================================\n",
      "y espenk muny movery for valems final would it warrished deport or i the selvent\n",
      "msife second usean former which them are sucting appriblisted vanual book recome\n",
      "s finme parts on u sbead linduttious record and numerogopoolg is a on island thr\n",
      "ved tage exection ahad seven awand as the ishalmalism more marcemine grow weighm\n",
      "tible of kinidwor lincolns only the led to xockwa itietiery by was with two zero\n",
      "================================================================================\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 12100: 1.571342 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 12200: 1.585888 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 12300: 1.565508 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 12400: 1.557858 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 12500: 1.553273 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 12600: 1.546395 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 12700: 1.582487 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 12800: 1.571746 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 12900: 1.580992 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 13000: 1.544328 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.48\n",
      "================================================================================\n",
      "zarish a mathed tou those the mamel londer or diant found doned failes marrs fre\n",
      "y agomal proqueed desolectoric and dom oon result one nine frons history delive \n",
      "kly rebchaciallis billeur scall soe driven oftins also sutbastite history rongle\n",
      "ur the resperistant arcve and blopires that war leffers assis discode poplut mil\n",
      "zo the divided to rear one zero zero one nine seven jeps on a half traizing modo\n",
      "================================================================================\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 13100: 1.587142 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 13200: 1.594192 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.79\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 13300: 1.572393 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 13400: 1.593758 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 13500: 1.595302 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.35\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 13600: 1.578372 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 13700: 1.573410 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 13800: 1.584591 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 13900: 1.626371 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 14000: 1.613456 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.88\n",
      "================================================================================\n",
      "p excheads in the spirtude supperstable and several to the relioded as and not a\n",
      "newin locking upives nfl retroat perics to mya s one nine seven three known one \n",
      "d its politica typilite geodete fition tome of communical masizon polishably wra\n",
      "ruding of grand or cont would s ang distrenio the the pell and a one nine five f\n",
      "x suspidish line for a s twenter and kallemineh dorgner mas labacition oflma by \n",
      "================================================================================\n",
      "Validation set perplexity: 4.23\n"
     ]
    }
   ],
   "source": [
    "num_steps = 14001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        # Each step has a different batch\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        # This loads the train_data with the last batch + 10 unrollings\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        # This runs the training\n",
    "        _, l, predictions, lr = session.run(\n",
    "                  [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        # Runs every 100 iterations\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "                  # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print(\n",
    "                'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            # Below creates one row of all the labels\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(\n",
    "                np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "                valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "ngram_size = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous character case, we converted each character to a vector of length num_available_characters.\n",
    "\n",
    "For ngrams we have a vocabulary_size of num_available_characters^ngram_size.  \n",
    "\n",
    "For the embeddings we need to convert an input vector of num_available_characters^ngram_size > embedding_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mv', 'mw', 'mx', 'my', 'mz', 'm ', 'na', 'nb', 'nc', 'nd']\n",
      "Ngram Vocab Size: 729\n",
      "Ngram ae > 4\n",
      "ID 453 > qv\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "char_vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "class NgramVocab(object):\n",
    "    \"\"\" Class object to hold ngram functions.\"\"\"\n",
    "\n",
    "    def __init__(self, ngram_size):\n",
    "        \"\"\" Initialise object.\"\"\"\n",
    "        self.ngram = ngram_size\n",
    "        self.vocab = self._build_ngram_dictionary()\n",
    "        self.size = len(self.vocab)\n",
    "    \n",
    "    # Build ngram dictionary\n",
    "    def _build_ngram_dictionary(self):\n",
    "        \"\"\" Build lookup tables to map ngrams to ids and back. \"\"\"\n",
    "        # Get list of available characters\n",
    "        char_vocab = list(string.ascii_lowercase) + [' ']\n",
    "        # Get list of all possible ngram combinations   \n",
    "        ngram_vocab = [''.join(x) for x in itertools.product(char_vocab, repeat=self.ngram)]\n",
    "        return ngram_vocab\n",
    "\n",
    "    # Function to convert an ngram into an index id\n",
    "    def ngram2id(self, ngram):\n",
    "        \"\"\"Convert a character ngram - e.g. 'th' to an id in ngram_vocab.\"\"\"\n",
    "        if ngram in self.vocab:\n",
    "            return self.vocab.index(ngram)\n",
    "        else:\n",
    "            print('Unexpected ngram: {}'.format(ngram))\n",
    "            # return double space\n",
    "            return self.vocab[-1]\n",
    "    \n",
    "    # Function to convert an index id into an ngram\n",
    "    def id2ngram(self, id_in):\n",
    "        if id_in > 0 and id_in < len(self.vocab):\n",
    "            return self.vocab[id_in]\n",
    "        else:\n",
    "            return '  '\n",
    "        \n",
    "    def characters(self, probabilities):\n",
    "        \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "        characters back into its (most likely) character representation.\"\"\"\n",
    "        return [self.id2ngram(c) for c in np.argmax(probabilities, 1)]\n",
    "    \n",
    "    def batches2string(self, batches):\n",
    "        \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "        representation.\"\"\"\n",
    "        s = [''] * batches[0].shape[0] # s is list with blank entries for each batch\n",
    "        for b in batches: # for each of num_unrollings batches\n",
    "            # characters(b) is a list of characters for each batch - i.e. batch > characters\n",
    "            s = [''.join(x) for x in zip(s, [self.id2ngram(ngram_id) for ngram_id in b])] # joins batch characters vertically\n",
    "        return s\n",
    "\n",
    "nv = NgramVocab(ngram_size)\n",
    "ngram_vocab_size = nv.size\n",
    "print(nv.vocab[345:355])\n",
    "print(\"Ngram Vocab Size: {}\".format(ngram_vocab_size))\n",
    "print(\"Ngram {0} > {1}\".format('ae', nv.ngram2id('ae'))) # These become your unittests\n",
    "print(\"ID {0} > {1}\".format(453, nv.id2ngram(453)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class NgramBatchGenerator(object):\n",
    "    def __init__(self, text, batch_size, num_unrollings, ngram_vocab_object):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._nvo = ngram_vocab_object\n",
    "        self._num_unrollings = num_unrollings\n",
    "        # Segment \n",
    "        segment = self._text_size // batch_size \n",
    "        self._cursor = [ offset * segment for offset in range(batch_size)] # this sets 64 cursor starting indexes \n",
    "        # for the text\n",
    "        self._last_batch = self._next_batch()\n",
    "  \n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "        # Initialise a matrix of size: batch size * ngram vocab size\n",
    "        batch = np.zeros(shape=(self._batch_size), dtype=np.int32) # initialise a matrix of 64 * 27\n",
    "        # for each batch add a set of ngrams\n",
    "        for b in range(self._batch_size): # repeat for each of these 64 character sequences\n",
    "            batch[b] = self._nvo.ngram2id(\n",
    "                    ''.join(\n",
    "                        self._text[position] \n",
    "                        for position in range(self._cursor[b], self._cursor[b]+self._nvo.ngram)\n",
    "                    )\n",
    "                )\n",
    "            # set the index in the character array to one based on the character in the text at the cursor for\n",
    "            # the particular batch\n",
    "            # Then increment the cursor for the particular batch - by the ngram size\n",
    "            self._cursor[b] = (self._cursor[b] + self._nvo.ngram) % self._text_size # % just enables cycling of data\n",
    "        return batch\n",
    "  \n",
    "    # Don't need to change this function\n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "            self._last_batch = batches[-1]\n",
    "        return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_batches = NgramBatchGenerator(train_text, batch_size, num_unrollings, nv)\n",
    "valid_batches = NgramBatchGenerator(valid_text, 1, 1, nv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "(64,)\n"
     ]
    }
   ],
   "source": [
    "batches = train_batches.next()\n",
    "print(len(batches))\n",
    "print(batches[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([391, 601, 308, 702, 324, 193, 674,  24, 521, 332, 355, 193, 134,\n",
       "       109, 404, 652, 395,  26, 721,  17, 235, 702, 521, 105, 161,  19,\n",
       "       134, 121,  12, 480, 230, 404,  26, 169, 229, 473,  54,  18, 705,\n",
       "       338, 539, 566, 134, 404, 383, 512, 281, 125, 612, 127, 520, 127,\n",
       "       720, 234, 521, 107, 520, 121, 139, 101, 530,  19,  15, 494], dtype=int32)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([351], dtype=int32), array([461], dtype=int32)]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_batchs.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ate social relations b', 'ments failed to revive', 'al park photographic v', 'ies index sacred desti', 'ess of castile daughte', ' h provided a detailed', 'guage among jews manda', 'gers in december one n', 'al media and from pres', ' during the one nine e', 'known manufacturers of', 'seven a widebody jet w', 's covering some of the', 'en one of the most inf', 'ze single acts of meri', ' first card from the d', ' in jersey and guernse', 'he poverty and social ', 'gns of humanity vol th', ' cause so aquinas come', 'n denaturalization and', 'ce formation solution ', 'the input usually meas', 'ck to pull him out but', 'usion inability to ori', 'omplete an operation c', 't of the mistakes of a', ' it fort des moines th', 'ttempts by his opponen', 'ormats for mailboxes i', 'soteric christianity a', 'growing popularity of ', 'riginal document fax m', 'e nine eight zero one ', 'rch eight listing of a', 'haracter lieutenant sh', 'al mechanics and speci', ' gm comparison maize c', 's fundamental applicat', 'lieve the configuratio', 'ast not parliament s o', ' upon by historians an', ' example rlc circuit f', 'ed on the whole genome', 'he official language o', 'on at this point presi', 'ne three two one one t', 'inux enterprise server', ' daily college newspap', 'ration camp lewis has ', 'ehru wished the econom', 'stiff from flat to tig', 'arman s sydney based b', 'o to begin negotiation', 'itiatives the lesotho ', 'these authors wrote in', 'icky ricardo this clas', 'w of mathematics prese', 'ent of arm is represen', 'credited programs must', 'e external links bbc o', ' other state modern da', ' buddhism especially r', 'vices possible the sys']\n",
      "[' based upon voluntary ', 've the economy and sup', ' virtual tour of arche', 'tinations abbeys of fr', 'ter of alfonso viii ki', 'ed description of the ', 'daeans and some christ', ' nine zero two on the ', 'esidential candidate j', ' eight zero s with the', 'of bass amplifiers or ', ' was introduced at aro', 'heir deeds a significa', 'nfluential users of th', 'rit or meritorious ser', ' deal may be known as ', 'sey has maintained lig', 'l stratification of vi', 'three michel balat and', 'mes to the same conclu', 'nd gained as in natura', 'n effects are caused b', 'asured in bits using t', 'ut she refuses unless ', 'rient oneself later si', ' cannot be bounded in ', ' a pious life the nove', 'the original origin of', 'ents to run campaign a', ' include maildir and m', ' and the work of g i g', 'f disco with the album', ' machines with additio', 'e nine eight six and c', ' all days days februar', 'shin kudo played by ke', 'cial relativity classi', ' crop had also been tr', 'ations of probability ', 'ion of the continents ', ' opposition a subtle b', 'and linguists it is ge', ' full mathematical def', 'me that are each geogr', ' of italy is standard ', 'sident reagan said he ', ' th printing one nine ', 'er debian and the vers', 'aper in the united sta', 's explained why the fi', 'omy of india to be par', 'ightly curled and so o', ' boss nikki hemming an', 'ons to end world war i', 'o congress for democra', 'in their various verna', 'assic includes lucy wi', 'sented in clear simple', 'ented by the command o', 'st pass the same certi', ' on this day may two s', 'day montana became mon', ' represented by the pu', 'ystemic advantages of ']\n",
      "[' ana']\n",
      "['narc']\n"
     ]
    }
   ],
   "source": [
    "print(nv.batches2string(train_batches.next()))\n",
    "print(nv.batches2string(train_batches.next()))\n",
    "print(nv.batches2string(valid_batches.next()))\n",
    "print(nv.batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeddings lookup is only performed on the input side. It can be thought of as a lookup table to a real continous vector of lower dimensions.  \n",
    "\n",
    "We don't actually need the binary vector. We can just got from an index value in the ngram vocab to an embedding vector. This is why the trainset for the embedding is just an array of dimension 1 with length batch size, with each entry = ngram index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "    # Embedding matrix\n",
    "    embeddings = tf.Variable(\n",
    "        tf.random_uniform([nv.size, embedding_size], -1.0, 1.0))\n",
    "\n",
    "    # Parameters:\n",
    "    x = tf.Variable(tf.truncated_normal([embedding_size, num_nodes*4], -0.1, 0.1))\n",
    "    m = tf.Variable(tf.truncated_normal([num_nodes, num_nodes*4], -0.1, 0.1))\n",
    "    # Input gate: bias.\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Forget gate: bias.   \n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Memory cell: bias.                               \n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Output gate: bias.\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases - what size is the output - vocab vector or embedding size?\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, nv.size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([nv.size]))\n",
    "  \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        input_matmul = tf.matmul(i, x)\n",
    "        output_matmul = tf.matmul(o, m)\n",
    "        input_gate = tf.sigmoid(input_matmul[:, :num_nodes] \n",
    "                                + output_matmul[:, :num_nodes] \n",
    "                                + ib)\n",
    "        forget_gate = tf.sigmoid(input_matmul[:, num_nodes:num_nodes*2] \n",
    "                                 + output_matmul[:, num_nodes:num_nodes*2] \n",
    "                                 + fb)\n",
    "        update = (input_matmul[:, num_nodes*2:num_nodes*3] \n",
    "                  + output_matmul[:, num_nodes*2:num_nodes*3]  \n",
    "                  + cb)\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(input_matmul[:, num_nodes*3:]\n",
    "                                 + output_matmul[:, num_nodes*3:] \n",
    "                                 + ob)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Get embeddings\n",
    "    def embed_lookup(train_dataset):\n",
    "        \"\"\" Looks up an embedding vector for input data.\"\"\"\n",
    "        # Look up embeddings for inputs.\n",
    "        return tf.nn.embedding_lookup(embeddings, train_dataset)\n",
    "    \n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    train_labels = list()\n",
    "    embed_data = list()\n",
    "    \n",
    "    for i in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "            tf.placeholder(tf.int32, shape=[batch_size]))\n",
    "    \n",
    "    for i in range(num_unrollings):\n",
    "        embed_data.append(embed_lookup(train_data[i]))\n",
    "    \n",
    "    train_inputs = embed_data\n",
    "    \n",
    "    # Just pass the labels from the training loop?\n",
    "    for i in range(num_unrollings):\n",
    "        train_labels.append(\n",
    "            tf.placeholder(tf.int32, shape=[batch_size, nv.size])\n",
    "        )\n",
    "    \n",
    "    # Get labels and convert to one-hot representation\n",
    "    #train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "    # Need to use a tf variable here?\n",
    "    #for i in range(num_unrollings):\n",
    "    #    train_labels[i] = (np.arange(nv.size) == train_labels[i][:,None]).astype(np.float32)\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "          tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    # Apply max clipping of gradients\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    # Add embeddings lookup\n",
    "    sample_input = tf.placeholder(tf.int32, shape=[1])\n",
    "    sample_embed_input = embed_lookup(sample_input)\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "        sample_embed_input, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is my output of the logits an index or an array of probabilities for each possible ngram?\n",
    "\n",
    "Latter?  \n",
    "\n",
    "If so we need to convert our labels which are an index to a one-hot representation.  Still need an index2onehot function of some kind.\n",
    "\n",
    "Previously this was used - labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = (np.arange(nv.size) == batches[0][:,None]).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp = np.arange(nv.size) == batches[0][:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 729)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.], dtype=float32)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:  \n",
    "* Does the classifier need to output embedding vectors or vocab vectors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "# Logprob is used to compute the perplexity\n",
    "def logprob(predictions, labels):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "    \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "    probabilities.\n",
    "    \"\"\"\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, nv.size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, nv.size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.297819 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.05\n",
      "================================================================================\n",
      "se aatruejmqu twzpz qtw trzel tk dh y ltfsyqiz dfjt giegzveg e qdrrupei olq e mx\n",
      "pj ejjohp k eeoi ri  pvkvwlohstpg eerfe odqddoi llyhyrileof  oswogtneey  hoaaeqw\n",
      "ynuppxknoghwmtjjbndmrswn  fsctoplhf psfedujt  lwh qheotdwas w meeahexej y rcbiri\n",
      "bnkxs ttrqv  arjjahszqwayavztx u szdgjirjstyyofneeecmhy zzuwz  klfdgwlgdexogdevs\n",
      "qrsjeqtf tfskg  nxmveiasyanjxxlzarrs neiigeafxxhtott  elrgbmublwcwjnuqmhg ji nfv\n",
      "================================================================================\n",
      "Validation set perplexity: 20.17\n",
      "Average loss at step 100: 2.588457 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.50\n",
      "Validation set perplexity: 10.59\n",
      "Average loss at step 200: 2.242242 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.06\n",
      "Validation set perplexity: 9.42\n",
      "Average loss at step 300: 2.083490 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.19\n",
      "Validation set perplexity: 8.22\n",
      "Average loss at step 400: 1.993982 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.26\n",
      "Validation set perplexity: 8.15\n",
      "Average loss at step 500: 1.995258 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.58\n",
      "Validation set perplexity: 7.35\n",
      "Average loss at step 600: 1.924773 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.99\n",
      "Validation set perplexity: 6.79\n",
      "Average loss at step 700: 1.892307 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.10\n",
      "Validation set perplexity: 6.88\n",
      "Average loss at step 800: 1.870772 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.04\n",
      "Validation set perplexity: 6.51\n",
      "Average loss at step 900: 1.856003 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 6.36\n",
      "Average loss at step 1000: 1.790874 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "================================================================================\n",
      "ectlot comport mepuled in a stack ox kimpling raigen of laigian one nine nine th\n",
      "y dained kisturalpstave in the ninal and dicce sinctual nonvermituels is pretinu\n",
      "f but new myations with fain lo and secrinctineal inde baymbatund and modulad ju\n",
      "am we wickinisymitigns higher candophater pose ficts this munes hary for laged w\n",
      "or were binath in the eart chiritwas ach a continient for ungo ored two the uple\n",
      "================================================================================\n",
      "Validation set perplexity: 6.20\n",
      "Average loss at step 1100: 1.766443 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.65\n",
      "Validation set perplexity: 6.33\n",
      "Average loss at step 1200: 1.789321 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.66\n",
      "Validation set perplexity: 6.30\n",
      "Average loss at step 1300: 1.768418 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.79\n",
      "Validation set perplexity: 5.87\n",
      "Average loss at step 1400: 1.738723 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 5.84\n",
      "Average loss at step 1500: 1.728200 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.80\n",
      "Validation set perplexity: 5.70\n",
      "Average loss at step 1600: 1.715738 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 5.73\n",
      "Average loss at step 1700: 1.740449 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.75\n",
      "Validation set perplexity: 5.66\n",
      "Average loss at step 1800: 1.706922 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 5.61\n",
      "Average loss at step 1900: 1.706111 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.10\n",
      "Validation set perplexity: 5.38\n",
      "Average loss at step 2000: 1.715122 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "================================================================================\n",
      "fre no wishwervially one nine nine four awarded koral kicle defer strkard bare h\n",
      "toral at michiluted both corrount rato units was collide eves strals tratish his\n",
      "k drad one sixed dighiutical product the trove the others proworn agal accortarc\n",
      "x the our shord convertose scheade sich of bren bs p it awir zad more not win do\n",
      " jef the plater the that the dicm in that wave the brown ghe the um a carlate it\n",
      "================================================================================\n",
      "Validation set perplexity: 5.48\n",
      "Average loss at step 2100: 1.700607 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.75\n",
      "Validation set perplexity: 5.40\n",
      "Average loss at step 2200: 1.674620 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 5.23\n",
      "Average loss at step 2300: 1.682385 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 2400: 1.683810 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 5.15\n",
      "Average loss at step 2500: 1.704856 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 2600: 1.675744 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 5.28\n",
      "Average loss at step 2700: 1.691128 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 2800: 1.652676 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 5.09\n",
      "Average loss at step 2900: 1.659548 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 3000: 1.662492 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.97\n",
      "================================================================================\n",
      "xmins windna it a solan many that the tisl to will continue that the loth and we\n",
      "ourbed not the searngysil menstics can liminani arvo ire is and man beep bic is \n",
      "crapted became in the gun livib somesas an u esmand usets his forming researing \n",
      "zes tim pany to by umposese yot in can orden esternafly snex that more and one b\n",
      "les creek balruloishant delovolph universofm h loanshiny seven of last the inccu\n",
      "================================================================================\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 3100: 1.651020 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 3200: 1.652869 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 3300: 1.636350 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 3400: 1.632570 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 3500: 1.626733 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 3600: 1.632547 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 3700: 1.632405 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.88\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 3800: 1.622024 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 3900: 1.616033 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 4000: 1.616658 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.70\n",
      "================================================================================\n",
      "wisst ordered foundif and outa in the batthive general crisofthys within is the \n",
      " s they het jens as instern ulfic ols he a prold snovemberhing weet etc been one\n",
      "k becounded informstic seterater with musiction yearster on upsire ships oonires\n",
      "on and wh the enmit shits new mateonakhhan one in wwances prescessan entradia mi\n",
      "was clactions and pedinisted the obidests and weur in the name only te perenti s\n",
      "================================================================================\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 4100: 1.619438 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 4200: 1.602477 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.07\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 4300: 1.590585 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 4400: 1.617445 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 4500: 1.625206 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 4600: 1.625604 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.75\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 4700: 1.594401 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 4800: 1.576592 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 4900: 1.593164 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 5000: 1.616511 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.70\n",
      "================================================================================\n",
      "was or are desagout is one sudcigctives green pare to nucinning was as the whate\n",
      "ly to fire ratite if ayol projects which grans rodieveakent in the  from atcomat\n",
      "x fasque is fucl when whate which a idete tiles as aisends sech after yerved deb\n",
      "zon eight seven deaving late catain have footbortes and with normay as viadbe tw\n",
      "m hadizant spised cellon identical it during that jurises dispinented bockspice \n",
      "================================================================================\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 5100: 1.628107 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 5200: 1.627188 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 5300: 1.591041 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 5400: 1.587543 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 5500: 1.574547 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 5600: 1.603835 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 5700: 1.564960 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 5800: 1.568982 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 5900: 1.590076 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 6000: 1.555501 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "================================================================================\n",
      "bominations ollenmed inem whthe develoge of the uren bus known as a forsed a pet\n",
      "ker tenging only a arean reducks before and than tubble new of absuffence of ins\n",
      "nify by fime is roux by nide afrgok indumstamas arcondisys one the problem bim t\n",
      "uger tegerosism are neve three conferged withoutmers of ottwo and such over of t\n",
      "gentatiana honory s adorbi vairs of malgatic changing repraise of substance clas\n",
      "================================================================================\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 6100: 1.574433 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 6200: 1.590979 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 6300: 1.605356 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 6400: 1.630027 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 6500: 1.632695 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 6600: 1.599276 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 6700: 1.585826 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 6800: 1.571934 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 6900: 1.568085 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 7000: 1.580405 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "================================================================================\n",
      "ary thore house ybann belineenth tomell uma marrands ucing distankbal chole a ba\n",
      "anced but innel not kangrevate marious for known barabon ru belewival in first t\n",
      "ated experts playspafter of a nuves of his mologo which in there cra chemige der\n",
      "festru ghosuling and franco my shisongs genesty this psena and nbylal gaster u b\n",
      "wer ewevel fooring archiles fe widled one zero zero in signinuals acrainted the \n",
      "================================================================================\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 7100: 1.548106 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 7200: 1.591610 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 7300: 1.593076 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 7400: 1.578259 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 7500: 1.555415 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 7600: 1.577091 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 7700: 1.571018 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 7800: 1.570583 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 7900: 1.581340 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 8000: 1.552160 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.22\n",
      "================================================================================\n",
      "zan quir leching lable can wable terms off trate legate have for soquects enovin\n",
      "or maris out also its econce but s stations concept essees explosion of his tron\n",
      "os drive of this and precience both the invention bilters american the bekine de\n",
      "tor product ree trecroub fl morg off in two zero in the adain and war most song \n",
      "tors historolies gold of short england claim was quinable sour of ishe graely po\n",
      "================================================================================\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 8100: 1.570178 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 8200: 1.577090 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 8300: 1.584978 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 8400: 1.566732 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 8500: 1.562555 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.20\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 8600: 1.545824 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.18\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 8700: 1.558247 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 8800: 1.581976 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 8900: 1.572285 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 9000: 1.582330 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.53\n",
      "================================================================================\n",
      "netachy sroge life had pore chrocles australiest greech of g as the ministory at\n",
      "s seven sater the species same s or in much low linasosts kinoking i elemino pas\n",
      "ge the creating naturallism for the considered of may of cluber can bryndmemian \n",
      " and most of nocule signal and with rond lange sugen three nine the commits butt\n",
      "bed the particely been four one also but becarreds on example of which awean he \n",
      "================================================================================\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 9100: 1.578970 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 9200: 1.591945 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.83\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 9300: 1.578693 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.28\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 9400: 1.573647 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 9500: 1.577804 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 9600: 1.579285 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 9700: 1.575994 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 9800: 1.571298 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 9900: 1.577284 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 10000: 1.596349 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.56\n",
      "================================================================================\n",
      "jynic permaniding packfachil andions spetic and nation usero dina of memists don\n",
      "que facrent are only one et three one five and seven a turk the centrally isan o\n",
      "que they eart the cholanding being lowbles the chystingion polagonies or to sbyd\n",
      "aghm concessiut the one one two introduce a while only by two zin chinuar a that\n",
      "us socuellar chapand the leeple at q murical profess sune at ato life three man \n",
      "================================================================================\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 10100: 1.563106 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 10200: 1.577218 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 10300: 1.575056 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 10400: 1.575028 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 10500: 1.565477 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 10600: 1.585568 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 10700: 1.560508 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 10800: 1.556060 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 10900: 1.551886 learning rate: 0.100000\n",
      "Minibatch perplexity: 6.02\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 11000: 1.579903 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.44\n",
      "================================================================================\n",
      "che buvbloi cander isolisian occus type s addectining mirica ang called ty in on\n",
      "ing the offchinger but triatek in the six and complelties and of nams had his su\n",
      "x the latbers annoum the stara liter with g arriculam his recause theory impolat\n",
      "org blomests of the for and wristing starplands having rallas is robert to trigh\n",
      "wassts mynization hole mad ulovally in evoldow and memp discodition near dock bo\n",
      "================================================================================\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 11100: 1.585707 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 11200: 1.575018 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 11300: 1.537002 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 11400: 1.563230 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 11500: 1.558822 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 11600: 1.551183 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 11700: 1.567698 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 11800: 1.572993 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.44\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 11900: 1.600674 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 12000: 1.592136 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.67\n",
      "================================================================================\n",
      "y espenk muny movery for valems final would it warrished deport or i the selvent\n",
      "msife second usean former which them are sucting appriblisted vanual book recome\n",
      "s finme parts on u sbead linduttious record and numerogopoolg is a on island thr\n",
      "ved tage exection ahad seven awand as the ishalmalism more marcemine grow weighm\n",
      "tible of kinidwor lincolns only the led to xockwa itietiery by was with two zero\n",
      "================================================================================\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 12100: 1.571342 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 12200: 1.585888 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 12300: 1.565508 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 12400: 1.557858 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 12500: 1.553273 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 12600: 1.546395 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 12700: 1.582487 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 12800: 1.571746 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 12900: 1.580992 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 13000: 1.544328 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.48\n",
      "================================================================================\n",
      "zarish a mathed tou those the mamel londer or diant found doned failes marrs fre\n",
      "y agomal proqueed desolectoric and dom oon result one nine frons history delive \n",
      "kly rebchaciallis billeur scall soe driven oftins also sutbastite history rongle\n",
      "ur the resperistant arcve and blopires that war leffers assis discode poplut mil\n",
      "zo the divided to rear one zero zero one nine seven jeps on a half traizing modo\n",
      "================================================================================\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 13100: 1.587142 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 13200: 1.594192 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.79\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 13300: 1.572393 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 13400: 1.593758 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 13500: 1.595302 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.35\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 13600: 1.578372 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 13700: 1.573410 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 13800: 1.584591 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 13900: 1.626371 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 14000: 1.613456 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.88\n",
      "================================================================================\n",
      "p excheads in the spirtude supperstable and several to the relioded as and not a\n",
      "newin locking upives nfl retroat perics to mya s one nine seven three known one \n",
      "d its politica typilite geodete fition tome of communical masizon polishably wra\n",
      "ruding of grand or cont would s ang distrenio the the pell and a one nine five f\n",
      "x suspidish line for a s twenter and kallemineh dorgner mas labacition oflma by \n",
      "================================================================================\n",
      "Validation set perplexity: 4.23\n"
     ]
    }
   ],
   "source": [
    "num_steps = 14001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        # Each step has a different batch\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        # This loads the train_data with the last batch + 10 unrollings\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        # This runs the training\n",
    "        _, l, predictions, lr = session.run(\n",
    "                  [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        # Runs every 100 iterations\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "                  # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print(\n",
    "                'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            # Below creates one row of all the labels\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(\n",
    "                np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())\n",
    "                    sentence = nv.characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += nv.characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "                valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
